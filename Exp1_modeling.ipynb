{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n========================================================\\nAuthor:  Sevan Harootonian\\nAffiliation: Princeton University\\nDate: 2025-08-20\\n========================================================\\n\\nDescription:\\n------------\\nThis notebook includes the following computations for experiment 1\\n  - Model simulations to compute utility (about 30min) [will use about 10gb of ram]\\n  - Model fitting to the data (about 5 min)\\n  - Model simulation using the fitted parameters (100 iteration ~20min)\\n  - Model fitting for model recovery (100 iteration ~ 14 hours)\\n  \\n========================================================\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "========================================================\n",
    "Author:  Sevan Harootonian\n",
    "Affiliation: Princeton University\n",
    "Date: 2025-08-20\n",
    "========================================================\n",
    "\n",
    "Description:\n",
    "------------\n",
    "This notebook includes the following computations for experiment 1\n",
    "  - Model simulations to compute utility (about 30min) [will use about 10gb of ram]\n",
    "  - Model fitting to the data (about 5 min)\n",
    "  - Model simulation using the fitted parameters (100 iteration ~20min)\n",
    "  - Model fitting for model recovery (100 iteration ~ 14 hours)\n",
    "  \n",
    "========================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model simulation for exp 1 (this wil take about 30min) about 10gb of ram\n",
    "\n",
    "import pandas as pd\n",
    "from functions.mdp_params import make_true_graph,create_true_mdp_params\n",
    "from functions.mentor import OptimalBayesianMentor, NaiveBayesianMentor, PriorOnlyMentor\n",
    "from functions.max_utility_models import q_values, pathAvgUtility\n",
    "from msdm.algorithms import ValueIteration\n",
    "from functions.functions import create_mdp\n",
    "from functions.graphWorld import coordToint,intTocoord\n",
    "from functions.features import get_feature_levels, get_feature_reward_sum\n",
    "\n",
    "sim = pd.DataFrame(pd.read_pickle('data/tasksetup/exp1_simtrials.pkl'))\n",
    "sim['OBM_advice_utility'] = [[0,0]]*len(sim)\n",
    "sim['NBM_advice_utility'] = [[0,0]]*len(sim)\n",
    "sim['POM_advice_utility'] = [[0,0]]*len(sim)\n",
    "sim['PathAvgUtility'] = [[0,0]]*len(sim)\n",
    "sim['Q-values'] = [[0,0]]*len(sim)\n",
    "sim['feature_levels'] = [[0,0]]*len(sim)\n",
    "sim['feature_reward_sum'] = [[0,0]]*len(sim)\n",
    "\n",
    "for i,row in sim.iterrows():\n",
    "    trial_param = create_true_mdp_params(0,4)\n",
    "    trial_param['goal_values'] = row.learner_rewards\n",
    "    obm_mentor = OptimalBayesianMentor(trial_param) # create Optimal Bayesian Mentor class\n",
    "    OBM_advice_utility = obm_mentor.advice_dist([row.traj,]) # adivce utility based on OBM\n",
    "\n",
    "    # trial_param = create_true_mdp_params(0,4)\n",
    "    # trial_param['goal_values'] = row.learner_reward\n",
    "    nbm_mentor = NaiveBayesianMentor(trial_param) \n",
    "    NBM_advice_utility = nbm_mentor.advice_dist([row.traj,]) \n",
    "\n",
    "    # trial_param = create_true_mdp_params(0,4)\n",
    "    # trial_param['goal_values'] = row.learner_rewards\n",
    "    pom_mentor = PriorOnlyMentor(trial_param)\n",
    "    POM_advice_utility = pom_mentor.prior_advice_dist()\n",
    "    \n",
    "    PathAvgUtility = pathAvgUtility(trial_param['connections'],trial_param['goal_values'])\n",
    "    \n",
    "    mdp = create_mdp(trial_param)\n",
    "    results = ValueIteration().plan_on(mdp=mdp)\n",
    "    Q_value = q_values(results.Q, trial_param['connections'])\n",
    "    \n",
    "\n",
    "    feature_level = coordToint(get_feature_levels(trial_param['connections']))\n",
    "    feature_reward = coordToint(get_feature_reward_sum(trial_param['goal_values'],trial_param['connections']))\n",
    "\n",
    "    #save as sim[,OBM_advice_utility]\n",
    "    sim.at[i, 'OBM_advice_utility'] = OBM_advice_utility\n",
    "    sim.at[i, 'NBM_advice_utility'] = NBM_advice_utility\n",
    "    sim.at[i, 'POM_advice_utility'] = POM_advice_utility\n",
    "    sim.at[i,'feature_levels'] = intTocoord(feature_level)\n",
    "    sim.at[i,'feature_reward_sum'] = intTocoord(feature_reward)\n",
    "    sim.at[i,'PathAvgUtility'] = PathAvgUtility\n",
    "    sim.at[i,'Q-values'] = Q_value\n",
    "\n",
    "\n",
    "pd.to_pickle(sim, 'data/sim/exp1/exp1_modelsim.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:00<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions.fitting import fitting_choices\n",
    "\n",
    "preprocessed = pd.read_pickle('data/preprocessed/exp1/preprocessed_exp1.pkl')\n",
    "\n",
    "models = {\n",
    "    \"PathAvgUtility\": [\"PathAvgUtility\"],\n",
    "    \"Q-values\": [\"Q-values\"],\n",
    "    \"Level\": [\"feature_levels\"],\n",
    "    \"Reward\": [\"feature_reward_sum\"],\n",
    "    \"Level,Reward\": [\"feature_levels\", \"feature_reward_sum\"],\n",
    "    \"OBM\": [\"OBM_AU\"],\n",
    "    \"NBM\": [\"NBM_AU\"],\n",
    "    \"POM\": [\"POM_AU\"],\n",
    "}\n",
    "\n",
    "df_fits_exp1 = fitting_choices(preprocessed, models,exp = 1)\n",
    "df_fits_exp1.to_pickle('data/preprocessed/exp1/df_fits_exp1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom functions.model_comparison import sample_multinomial_logit\nfrom collections import defaultdict\nfrom functions.functions import max_value_keys\n            \nsimdata = pd.DataFrame(pd.read_pickle('data/sim/exp1_modelsim.pickle'))\nrename_simdata_map = {\n    \"feature_levels\": \"Level\",\n    \"feature_reward_sum\": \"Reward\",\n    \"OBM_advice_utility\": \"OBM\",\n    \"NBM_advice_utility\": \"NBM\",\n    \"POM_advice_utility\": \"POM\",\n}\nsimdata.rename(columns=rename_simdata_map, inplace=True)\n\ndf_fits = pd.read_pickle('data/preprocessed/exp1/df_fits_exp1.pkl')\nrename_df_fits_map = {\n    \"PathAvgUtility\": \"PathAvgUtility\",\n    \"Q-values\": \"Q-values\",\n    \"feature_levels\": \"Level\",\n    \"feature_reward_sum\": \"Reward\",\n    \"OBM_AU\": \"OBM\",\n    \"NBM_AU\": \"NBM\",\n    \"POM_AU\": \"POM\",\n}\ndf_fits.rename(columns=rename_df_fits_map,inplace=True)\n\nmodels = {\n    \"PathAvgUtility\": [\"PathAvgUtility\"],\n    \"Q-values\": [\"Q-values\"],\n    \"Level\": [\"Level\"],\n    \"Reward\": [\"Reward\"],\n    \"Level,Reward\": [\"Level\", \"Reward\"],\n    \"OBM\": [\"OBM\"],\n    \"NBM\": [\"NBM\"],\n    \"POM\": [\"POM\"],\n}\n\nposterior_sim = pd.DataFrame({\"samples\": [],\n                              \"subjID\": [],\n                              \"model\": [],\n                              \"feature_weights\": [],\n                              \"choice\": [],\n                              \"teaching_score\" : [],\n                            })\nsimrows = []\n\nSubjects = df_fits.subjID.unique()\nK = df_fits.model.unique()\niteration = 100\n\nfor s in range(0,iteration):\n    for subj in Subjects:\n        subj_fits =  df_fits[df_fits.subjID == subj].reset_index(drop=True)\n        for k in K:\n            feature_weights = subj_fits[subj_fits.model == k][models[k]].values.flatten()\n            for i, row in simdata.iterrows():\n                feature_value_dict = row[models[k]]\n                choice = sample_multinomial_logit(feature_weights,feature_value_dict)\n                teaching_score = row.OBM[choice]/ row.OBM[max_value_keys(row.OBM)[0]] \n            \n                simrow= {\n                    \"iteration\": s,\n                    \"subjID\": subj,\n                    \"sim_model\": k,\n                    'trial_id' : row.seed,\n                    \"feature_weights\": feature_weights.tolist(),  \n                    \"choice\": choice,\n                    \"teaching_score\": teaching_score,\n                }\n                for model_name in models.keys():\n                    if model_name != \"Level,Reward\":\n                        simrow[model_name] = row[model_name] \n\n                simrows.append(simrow)\n\nposterior_sim = pd.DataFrame.from_records(simrows)\n\n# Split into two halves to reduce file size for GitHub (files > 100MB not allowed)\nposterior_sim_0to50 = posterior_sim[posterior_sim['iteration'] < 50]\nposterior_sim_50to100 = posterior_sim[posterior_sim['iteration'] >= 50]\n\n# Save the two halves\nposterior_sim_0to50.to_pickle('data/sim/exp1/exp1_posterior_sim_0to50.pkl')\nposterior_sim_50to100.to_pickle('data/sim/exp1/exp1_posterior_sim_50to100.pkl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, functions.fitting\n",
    "importlib.reload(functions.fitting)\n",
    "from functions.fitting import model_recovery_fitting\n",
    "\n",
    "sim_fits = model_recovery_fitting(posterior_sim, models)\n",
    "\n",
    "sim_fits.to_pickle('data/sim/exp1/exp1_simfits_100.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}